"""Vertex AI post-processing module for transcript formatting."""

import datetime
from typing import Optional

from ..config import settings, VertexAIModels
from ..utils.colors import Colors
from ..utils.chunking import TranscriptChunker


class VertexPostProcessor:
    """Vertex AI (Gemini) post-processor for script-style formatting."""
    
    def __init__(self):
        self.project_id = settings.vertex_project_id
        self.chunker = TranscriptChunker()
    
    def process(self, transcript_text: str, video_title: str = "", vertex_ai_model: str = "") -> Optional[str]:
        """
        Post-process transcript using Vertex AI (Gemini) for script formatting.
        Supports both single-pass and chunked processing for long transcripts.
        
        Args:
            transcript_text: Original transcript text
            video_title: Video title
            vertex_ai_model: Specific model to use
            
        Returns:
            Formatted script text or None on error
        """
        print(Colors.BLUE + "\nğŸ¤– Vertex AI utÃ³feldolgozÃ¡s indÃ­tÃ¡sa..." + Colors.ENDC)
        
        # Check if chunking is needed
        if self.chunker.needs_chunking(transcript_text):
            return self._process_with_chunking(transcript_text, video_title, vertex_ai_model)
        else:
            return self._process_single_chunk(transcript_text, video_title, vertex_ai_model)
    
    def _process_with_chunking(self, transcript_text: str, video_title: str, vertex_ai_model: str) -> Optional[str]:
        """Process long transcript using chunking strategy."""
        print(Colors.YELLOW + f"ğŸ“‘ HosszÃº Ã¡tirat Ã©szlelve ({len(transcript_text)} karakter)" + Colors.ENDC)
        
        # Show chunking summary
        chunk_summary = self.chunker.get_chunk_summary(transcript_text)
        print(Colors.CYAN + f"   â”œâ”€ {chunk_summary}" + Colors.ENDC)
        
        # Get chunks
        chunks = self.chunker.chunk_text(transcript_text)
        print(Colors.CYAN + f"   â”œâ”€ {len(chunks)} chunk lÃ©trehozva" + Colors.ENDC)
        
        try:
            import vertexai
            from vertexai.generative_models import GenerativeModel, GenerationConfig
            
            # Process each chunk
            processed_chunks = []
            successful_config = None
            
            for i, (chunk_text, start_pos, end_pos) in enumerate(chunks):
                print(Colors.CYAN + f"   â”œâ”€ Chunk {i+1}/{len(chunks)} feldolgozÃ¡sa ({len(chunk_text)} kar.)" + Colors.ENDC)
                
                # Use single chunk processing for each chunk
                result = self._process_single_chunk_internal(chunk_text, vertex_ai_model)
                if result is None:
                    print(Colors.WARNING + f"   âœ— Chunk {i+1} feldolgozÃ¡sa sikertelen" + Colors.ENDC)
                    return None
                
                processed_chunks.append(result)
                print(Colors.GREEN + f"   âœ“ Chunk {i+1} kÃ©sz" + Colors.ENDC)
            
            # Merge results
            print(Colors.CYAN + "   â”œâ”€ Chunk-ok egyesÃ­tÃ©se..." + Colors.ENDC)
            merged_text = self.chunker.merge_chunked_results(processed_chunks, chunks)
            
            # Build final result with chunk information
            final_text = self._build_final_result_chunked(
                merged_text, video_title, transcript_text, 
                vertex_ai_model, len(chunks)
            )
            
            print(Colors.GREEN + f"   âœ“ Chunked feldolgozÃ¡s kÃ©sz: {len(chunks)} chunk Ã¶sszevonva" + Colors.ENDC)
            return final_text
            
        except ImportError:
            print(Colors.WARNING + "âš  Vertex AI kÃ¶nyvtÃ¡r nincs telepÃ­tve!" + Colors.ENDC)
            return self._fallback_processing(transcript_text, video_title)
        except Exception as e:
            print(Colors.FAIL + f"âœ— Chunked feldolgozÃ¡s hiba: {e}" + Colors.ENDC)
            return self._fallback_processing(transcript_text, video_title)
    
    def _process_single_chunk(self, transcript_text: str, video_title: str, vertex_ai_model: str) -> Optional[str]:
        """Process transcript as single chunk (original behavior)."""
        try:
            import vertexai
            from vertexai.generative_models import GenerativeModel, GenerationConfig
            
            print(Colors.CYAN + f"   â”œâ”€ Project: {self.project_id}" + Colors.ENDC)
            
            result = self._process_single_chunk_internal(transcript_text, vertex_ai_model)
            if result is None:
                return None
                
            # Build final result 
            final_text = self._build_final_result(result, video_title, transcript_text, vertex_ai_model)
            return final_text
            
        except ImportError:
            print(Colors.WARNING + "âš  Vertex AI kÃ¶nyvtÃ¡r nincs telepÃ­tve!" + Colors.ENDC)
            return self._fallback_processing(transcript_text, video_title)
        except Exception as e:
            print(Colors.FAIL + f"âœ— Vertex AI hiba: {e}" + Colors.ENDC)
            return self._fallback_processing(transcript_text, video_title)
    
    def _process_single_chunk_internal(self, chunk_text: str, vertex_ai_model: str) -> Optional[str]:
        """Internal method to process a single chunk of text."""
        try:
            import vertexai
            from vertexai.generative_models import GenerativeModel, GenerationConfig
            
            # Determine which models to try
            if vertex_ai_model and vertex_ai_model != VertexAIModels.AUTO_DETECT:
                models_to_try = [vertex_ai_model] + VertexAIModels.get_auto_detect_order()
            else:
                models_to_try = VertexAIModels.get_auto_detect_order()
            
            # Try different regions
            regions = ["us-central1", "us-east1", "us-west1", "europe-west4"]
            
            for region in regions:
                for model_name in models_to_try:
                    try:
                        # Initialize Vertex AI with current region
                        vertexai.init(project=self.project_id, location=region)
                        model = GenerativeModel(model_name)
                        
                        # Create formatting prompt - use chunk_text directly
                        prompt = self._build_formatting_prompt(chunk_text)
                        
                        # Call Gemini API
                        response = model.generate_content(
                            prompt,
                            generation_config=GenerationConfig(
                                temperature=0.3,
                                max_output_tokens=8192,
                                top_p=0.8,
                            )
                        )
                        
                        return response.text
                        
                    except Exception as e:
                        continue
            
            raise Exception("Nem sikerÃ¼lt kapcsolÃ³dni egyetlen Gemini modellhez sem")
            
        except Exception as e:
            return None
    
    def _fallback_processing(self, transcript_text: str, video_title: str = "") -> str:
        """
        Fallback processing when Vertex AI is unavailable.
        Apply basic formatting without AI processing.
        """
        print(Colors.CYAN + "   â”œâ”€ EgyszerÅ± formÃ¡zÃ¡s alkalmazÃ¡sa" + Colors.ENDC)
        
        # Add header and basic formatting
        lines = transcript_text.split('\n')
        formatted_lines = []
        
        for line in lines:
            if line.strip():
                # Add proper capitalization and punctuation
                formatted_line = line.strip()
                if formatted_line and not formatted_line[0].isupper():
                    formatted_line = formatted_line[0].upper() + formatted_line[1:]
                if formatted_line and not formatted_line.endswith(('.', '!', '?')):
                    formatted_line += '.'
                formatted_lines.append(formatted_line)
            else:
                formatted_lines.append('')
        
        # Build final result
        formatted_text = '\n'.join(formatted_lines)
        final_text = self._build_final_result(formatted_text, video_title, transcript_text, "Fallback Format")
        
        print(Colors.GREEN + "   âœ“ Fallback formÃ¡zÃ¡s kÃ©sz" + Colors.ENDC)
        return final_text
    
    def _build_formatting_prompt(self, transcript_text: str) -> str:
        """Build the formatting prompt for Gemini."""
        # For chunked processing, we use the text as-is (already chunked appropriately)
        # For single-pass processing, we still apply the 5000 character limit
        if len(transcript_text) <= settings.chunk_size:
            limited_transcript = transcript_text
        else:
            limited_transcript = transcript_text[:settings.max_transcript_length]
        
        prompt = f"""FormÃ¡zd Ã¡t ezt a YouTube videÃ³ Ã¡tiratot professzionÃ¡lis SCRIPT/FELIRAT stÃ­lusÃºra!

EREDETI ÃTIRAT (Google Speech API):
{limited_transcript}

FORMÃZÃSI SZABÃLYOK:
1. MINDEN mondatot/tagmondatot kÃ¼lÃ¶n sorba, idÅ‘bÃ©lyeggel [HH:MM:SS] formÃ¡tumban
2. Maximum 12-15 szÃ³ soronkÃ©nt (Ã¡tlag beszÃ©dtempÃ³: 140 szÃ³/perc)
3. MondatvÃ©geknÃ©l (.!?) MINDIG Ãºj sor
4. Minden jelentÅ‘s szÃ¼netet jelÃ¶lj kÃ¼lÃ¶n sorban idÅ‘bÃ©lyeggel:
   [0:XX:XX] [rÃ¶vid szÃ¼net] = kb 0.5-1s csend
   [0:XX:XX] [levegÅ‘vÃ©tel] = kb 1-2s szÃ¼net
   [0:XX:XX] [hosszÃº szÃ¼net] = kb 2-3s szÃ¼net
   [0:XX:XX] [TÃ‰MAVÃLTÃS] = 3s+ szÃ¼net vagy Ãºj tÃ©ma
5. TermÃ©szetes beszÃ©dritmus kÃ¶vetÃ©se
6. KÃ¶tÅ‘szavaknÃ¡l (Ã©s, de, hogy, mert, hiszen) tÃ¶rj sort ha mÃ¡r 8+ szÃ³ van
7. VesszÅ‘knÃ©l tÃ¶rj sort ha a mondat mÃ¡r hosszÃº
8. BecsÃ¼ld meg reÃ¡lisan az idÅ‘bÃ©lyegeket (140 szÃ³/perc = kb 2.3 szÃ³/mp)

PÃ‰LDA KIMENETI FORMÃTUM:
[0:00:00] Sziasztok, szeretettel kÃ¶szÃ¶ntelek benneteket.
[0:00:03] [levegÅ‘vÃ©tel]
[0:00:04] Hoztam nektek a mai napra is egy Ã¼zenetet,
[0:00:07] egy Ã¡ltalÃ¡nos kÃ¡rtya olvasÃ¡st.
[0:00:09] [rÃ¶vid szÃ¼net]
[0:00:10] KÃ¡rtya kirakÃ¡s, itt lÃ¡tjÃ¡tok az asztalomon
[0:00:13] az univerzumnak az Ã¼zenetÃ©vel kezdenÃ©m,
[0:00:16] [levegÅ‘vÃ©tel]
[0:00:17] ami azt mondja Ã©s azt Ã¼zeni nektek,
[0:00:20] hogy jÃ³ lenne, hogyha Ã¡t adnÃ¡tok magatokat
[0:00:23] egy nÃ¡latok hatalmasabb ErÅ‘nek.
[0:00:26] [hosszÃº szÃ¼net]
[0:00:28] Az azt jelenti, hogy Ã¡tadom magam
[0:00:31] egy nÃ¡lam hatalmasabb ErÅ‘nek.

FONTOS: 
- LegalÃ¡bb 100-150 sor legyen a vÃ©geredmÃ©ny
- Minden szÃ¼netet jelÃ¶lj ahol Ã©rzed a beszÃ©dben
- A mondatok legyenek termÃ©szetesek, ne tÃ¶rj rossz helyen
- Az idÅ‘bÃ©lyegek legyenek reÃ¡lisak

FORMÃZOTT SCRIPT:"""
        
        return prompt
    
    def _build_final_result(self, formatted_text: str, video_title: str, 
                          original_transcript: str, model_name: str = "Gemini") -> str:
        """Build final result with header and statistics."""
        final_text = f"ğŸ“¹ VideÃ³: {video_title}\n"
        final_text += f"ğŸ“… Feldolgozva: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M')}\n"
        final_text += f"ğŸ¤– UtÃ³feldolgozÃ¡s: Vertex AI ({model_name})\n"
        final_text += "=" * 70 + "\n\n"
        final_text += formatted_text
        
        # Add note if transcript was truncated
        if len(original_transcript) > 5000:
            final_text += f"\n\n[FIGYELEM: A teljes Ã¡tirat {len(original_transcript)} karakter, csak az elsÅ‘ 5000 lett formÃ¡zva]"
        
        # Calculate statistics
        lines = formatted_text.split('\n')
        word_count = len(formatted_text.split())
        pause_count = formatted_text.count('[') - formatted_text.count('[0:')
        
        final_text += f"\n\n{'='*70}\n"
        final_text += f"ğŸ“Š Script statisztika (AI formÃ¡zÃ¡s):\n"
        final_text += f"   â€¢ Sorok szÃ¡ma: {len(lines)}\n"
        final_text += f"   â€¢ Ã–sszes szÃ³: {word_count}\n"
        final_text += f"   â€¢ DetektÃ¡lt szÃ¼netek: {pause_count}\n"
        final_text += f"   â€¢ Ãtlagos sorhossz: {word_count/len(lines) if lines else 0:.1f} szÃ³\n"
        
        return final_text
    
    def _build_final_result_chunked(self, formatted_text: str, video_title: str, 
                                  original_transcript: str, model_name: str, chunk_count: int) -> str:
        """Build final result for chunked processing with additional statistics."""
        final_text = f"ğŸ“¹ VideÃ³: {video_title}\n"
        final_text += f"ğŸ“… Feldolgozva: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M')}\n"
        final_text += f"ğŸ¤– UtÃ³feldolgozÃ¡s: Vertex AI ({model_name}) - Chunked Mode\n"
        final_text += f"ğŸ“‘ Chunks: {chunk_count} darab ({len(original_transcript)} â†’ {len(formatted_text)} karakter)\n"
        final_text += "=" * 70 + "\n\n"
        final_text += formatted_text
        
        # Calculate statistics
        lines = formatted_text.split('\n')
        word_count = len(formatted_text.split())
        pause_count = formatted_text.count('[') - formatted_text.count('[0:')
        
        final_text += f"\n\n{'='*70}\n"
        final_text += f"ğŸ“Š Chunked Script statisztika:\n"
        final_text += f"   â€¢ Eredeti hossz: {len(original_transcript)} karakter\n"
        final_text += f"   â€¢ Feldolgozott chunks: {chunk_count}\n"
        final_text += f"   â€¢ FormÃ¡zott sorok: {len(lines)}\n"
        final_text += f"   â€¢ Ã–sszes szÃ³: {word_count}\n"
        final_text += f"   â€¢ DetektÃ¡lt szÃ¼netek: {pause_count}\n"
        final_text += f"   â€¢ Ãtlagos sorhossz: {word_count/len(lines) if lines else 0:.1f} szÃ³\n"
        
        return final_text